{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Prototipo**"
      ],
      "metadata": {
        "id": "wIbk7Zxt_o-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OgCiZpHSEEd",
        "outputId": "6e57e465-3ea2-4e01-9935-37ea5588c529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/drive/My Drive/Proyecto 7/categorizador/predictor.py\"\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from joblib import dump, load\n",
        "import os\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.df = None\n",
        "        self.df_categorica = None\n",
        "        self.df_copy = None\n",
        "        self.df_encoded = None\n",
        "        self.df_concatenado = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.pca = None\n",
        "        self.load_data()  # Cargar datos al inicializar\n",
        "\n",
        "    def load_data(self):\n",
        "        try:\n",
        "            with open(self.data_path, 'rb') as f:\n",
        "                self.df = pickle.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: No se encontró el archivo en la ruta especificada: {self.data_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar el archivo pickle: {str(e)}\")\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        if self.df is None:\n",
        "            print(\"Error: No se ha cargado ningún DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Seleccionar columnas categóricas\n",
        "        self.df_categorica = self.df.select_dtypes(include=[\"object\", \"bool\"]).columns\n",
        "\n",
        "        # Eliminar columnas específicas si existen\n",
        "        columns_to_drop = ['biourl', 'movie', \"birthplace\", \"date_of_birth\", 'person', \"_last_judgment_at\", \"_unit_id\", \"birthplace:confidence\", \"date_of_birth:confidence\", \"race_ethnicity:confidence\", \"religion:confidence\", \"sexual_orientation:confidence\", \"year_of_award:confidence\"]\n",
        "        self.df_categorica = self.df_categorica.drop(columns_to_drop, errors='ignore')\n",
        "\n",
        "        # Hacer una copia del DataFrame original\n",
        "        self.df_copy = self.df.copy()\n",
        "\n",
        "        # Seleccionar columnas que no sean 'object' ni 'bool'\n",
        "        self.df_copy = self.df_copy.select_dtypes(exclude=['object', 'bool'])\n",
        "\n",
        "        # Obtener una copia del DataFrame codificado para las columnas categóricas\n",
        "        self.df_encoded = self.df[self.df_categorica].copy()\n",
        "\n",
        "        # Crear embeddings para cada columna categórica\n",
        "        category_embedding = {}\n",
        "        for column in self.df_encoded.select_dtypes(include=['object', 'bool']).columns:\n",
        "            unique_categories = self.df_encoded[column].unique()\n",
        "            category_embedding[column] = dict(zip(unique_categories, range(1, len(unique_categories) + 1)))\n",
        "            self.df_encoded[column] = self.df_encoded[column].map(category_embedding[column]).fillna(0)\n",
        "\n",
        "        # Para finalizar con la transformación de los datos, se concatenan los df: df_encoded con df_copy\n",
        "        self.df_concatenado = pd.concat([self.df_encoded, self.df_copy], axis=1)\n",
        "\n",
        "        # Separar características (X) y etiquetas (y)\n",
        "        self.X = self.df_concatenado.drop('gender', axis=1)\n",
        "        self.y = self.df_concatenado['gender']\n",
        "\n",
        "        # Dividir en entrenamiento y prueba\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Escalar características\n",
        "        self.scaler = StandardScaler()\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test = self.scaler.transform(self.X_test)\n",
        "\n",
        "        # Aplicar PCA\n",
        "        self.pca = PCA(n_components=8)  # Número óptimo de componentes\n",
        "        self.X_train = self.pca.fit_transform(self.X_train)\n",
        "        self.X_test = self.pca.transform(self.X_test)\n",
        "\n",
        "    def build_model(self):\n",
        "        if self.X_train is None or self.y_train is None:\n",
        "            print(\"Error: No se han dividido los datos en entrenamiento y prueba.\")\n",
        "            return\n",
        "\n",
        "        # Crear pipeline con StandardScaler, PCA y LogisticRegression\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('pca', PCA(n_components=8)),\n",
        "            ('logreg', LogisticRegression())\n",
        "        ])\n",
        "\n",
        "        # Definir el grid de hiperparámetros para LogisticRegression\n",
        "        param_grid = {\n",
        "            'logreg__C': [0.1, 1, 10, 100],\n",
        "            'logreg__solver': ['liblinear', 'lbfgs'],\n",
        "            'logreg__penalty': ['l2'],\n",
        "            'logreg__max_iter': [100, 200, 300]\n",
        "        }\n",
        "\n",
        "        # Configurar GridSearchCV con validación cruzada estratificada\n",
        "        grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='accuracy')\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        grid_search.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Evaluar el mejor modelo\n",
        "        self.model = grid_search.best_estimator_\n",
        "        y_pred = self.model.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        print(f'Accuracy: {accuracy}')\n",
        "        print(classification_report(self.y_test, y_pred))\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        try:\n",
        "            if self.model is None:\n",
        "                print(\"Error: No hay ningún modelo entrenado para guardar.\")\n",
        "                return\n",
        "\n",
        "            if not os.path.exists('saved_models'):\n",
        "                os.makedirs('saved_models')\n",
        "\n",
        "            model_path = os.path.join('saved_models', filename + '.joblib')\n",
        "            dump(self.model, model_path)\n",
        "\n",
        "            if self.scaler:\n",
        "                scaler_path = os.path.join('saved_models', filename + '_scaler.joblib')\n",
        "                dump(self.scaler, scaler_path)\n",
        "\n",
        "            if self.pca:\n",
        "                pca_path = os.path.join('saved_models', filename + '_pca.joblib')\n",
        "                dump(self.pca, pca_path)\n",
        "\n",
        "            print(f\"Modelo guardado correctamente en '{model_path}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al guardar el modelo: {str(e)}\")\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        try:\n",
        "            model_path = os.path.join('saved_models', filename + '.joblib')\n",
        "            self.model = load(model_path)\n",
        "\n",
        "            scaler_path = os.path.join('saved_models', filename + '_scaler.joblib')\n",
        "            if os.path.exists(scaler_path):\n",
        "                self.scaler = load(scaler_path)\n",
        "\n",
        "            pca_path = os.path.join('saved_models', filename + '_pca.joblib')\n",
        "            if os.path.exists(pca_path):\n",
        "                self.pca = load(pca_path)\n",
        "\n",
        "            print(f\"Modelo '{filename}' cargado correctamente.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: No se encontró el archivo en la ruta especificada: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar el modelo: {str(e)}\")\n",
        "\n",
        "# Ejemplo de uso:\n",
        "data_path = r'/content/drive/My Drive/Proyecto 7/data/df.pkl'  # Ruta hacia tu archivo pickle\n",
        "predictor = Predictor(data_path=data_path)\n",
        "predictor.preprocess_data()\n",
        "predictor.build_model()\n",
        "\n",
        "# Guardar el modelo\n",
        "predictor.save_model('logistic_regression_model')\n",
        "\n",
        "# Cargar el modelo\n",
        "loaded_predictor = Predictor(data_path=data_path)\n",
        "loaded_predictor.load_model('logistic_regression_model')\n",
        "# Ahora loaded_predictor.model está listo para hacer predicciones o evaluaciones."
      ],
      "metadata": {
        "id": "KNVCfmEzbeKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a21c159-2efc-4146-e908-80c56172e444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/My Drive/Proyecto 7/categorizador/predictor.py\n"
          ]
        }
      ]
    }
  ]
}